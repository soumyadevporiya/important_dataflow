{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8c33f0f9-25f8-4d72-881d-c491d66fd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Options first\n",
    "\n",
    "\n",
    "import google.auth\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "#from apache_beam.utils.pipeline_options import SetupOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import WorkerOptions\n",
    "import apache_beam as beam\n",
    "import time\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "\n",
    "options = PipelineOptions()\n",
    "\n",
    "#Setting the Options Programmatically\n",
    "options = PipelineOptions(flags=[])\n",
    "\n",
    "#set the project to the default project in your current Google Cloud Environment\n",
    "_, options.view_as(GoogleCloudOptions).project=google.auth.default()\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(StandardOptions).streaming = True\n",
    "options.view_as(GoogleCloudOptions).enable_streaming_engine = True\n",
    "#options.view_as(PipelineOptions).experiments = 'use_runner_v2' the log says it is by deafult\n",
    "\n",
    "#number of workers\n",
    "options.view_as(WorkerOptions).num_workers = 2\n",
    "\n",
    "#Set the Google Cloud Region in which Cloud Dataflow run\n",
    "options.view_as(GoogleCloudOptions).region='us-west1'\n",
    "\n",
    "#Cloud Storage Location\n",
    "dataflow_gcs_location='gs://gcp-dataeng-demos-soumya/dataflow'\n",
    "\n",
    "#Dataflow Staging Location. This location is used to stage the Dataflow Pipeline and SDK binary\n",
    "options.view_as(GoogleCloudOptions).staging_location = '{}/staging'.format(dataflow_gcs_location)\n",
    "#Dataflow Temp Location. This location is used to store temporary files or intermediate results before finally outputting\n",
    "options.view_as(GoogleCloudOptions).temp_location = '{}/temp'.format(dataflow_gcs_location)\n",
    "#The directory to store the output files of the job\n",
    "output_gcs_location = '{}/output'.format(dataflow_gcs_location)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4049d0-e78c-4c16-b821-3981722f0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################  With A Single Worker    ############################################################################\n",
    "\n",
    "\n",
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records`'\n",
    "\n",
    "\n",
    "\n",
    "def insert_key(x):\n",
    "    return (x['id'][0:6],(x['id'],x['name']))\n",
    "\n",
    "def makeRow(x):\n",
    "    row = dict(zip(('id','name'),x))\n",
    "    return row\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        #from kafka import KafkaProducer\n",
    "        \n",
    "        #producer = KafkaProducer(bootstrap_servers=['34.28.118.32:9094'], api_version=(0, 10))\n",
    "        \n",
    "        #content = urllib.request.urlopen('http://35.239.64.67:32726/').read().decode('utf-8')\n",
    "        #row = dict(zip(('message'),content))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # The API endpoint\n",
    "        #url = \"http://35.239.64.67:32726/\"\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts/1\"\n",
    "\n",
    "        # A GET request to the API\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Print the response\n",
    "        response_json = response.json()\n",
    "        #producer.send('my-topic', json.dumps(response_json).encode('utf-8'))\n",
    "        #if producer is not None:\n",
    "                    #producer.close()\n",
    "\n",
    "        length = len(element[1])\n",
    "        x = response_json['userId']\n",
    "        x1 = response_json['id']\n",
    "        x2 = response_json['title']\n",
    "        x3 = response_json['body']\n",
    "        #x=1\n",
    "        #x1=1\n",
    "        #x2='string'\n",
    "        #x3='strong'\n",
    "        \n",
    "        row = dict(zip(('userId','id','title','body','length'),(x,x1,x2,x3,length)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "          \n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "#with beam.Pipeline(options=options) as p:\n",
    "read_requests =(p| \n",
    "    beam.io.ReadFromBigQuery(query=query,use_standard_sql=True )\n",
    "    |beam.Map(insert_key)\n",
    "    |'Group into batches' >> beam.GroupIntoBatches(50000)\n",
    "    |'Make an External API Call' >> beam.ParDo(make_api_call())\n",
    "    |\"Write to Bigquery\">>beam.io.Write(beam.io.WriteToBigQuery(        'my-table-message-second',\n",
    "                                                                        dataset='gcpdataset',\n",
    "                                                                        project='mimetic-parity-378803',\n",
    "                                                                        schema='userId:INTEGER, id:INTEGER,title:STRING,body:STRING,length:INTEGER',\n",
    "                                                                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, #If table does not exist before, this code throws error\n",
    "                                                                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                                                                        \n",
    "                                                                      )\n",
    "                                                       )\n",
    "    )\n",
    "    #|beam.Map(print))\n",
    "               \n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c7d5b1b-69f2-4d92-8040-f0c0f268f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset mimetic-parity-378803:beam_temp_dataset_89019b0a4d6e4917b6434f1064498fd7 does not exist so we will create it as temporary with location=us-west1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('INSBI7', ('INSBI759226449', 'hthEUEqmx')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI7', ('INSBI759226449', 'hthEUEqmx')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI6', ('INSBI652587880', 'XqMkSHEva')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI6', ('INSBI652587880', 'XqMkSHEva')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI5', ('INSBI577879924', 'noNLOvNIo')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI5', ('INSBI577879924', 'noNLOvNIo')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI1', ('INSBI188022073', 'jhpicJqUN')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI1', ('INSBI188022073', 'jhpicJqUN')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI2', ('INSBI230887749', 'CajgKklNk')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI2', ('INSBI230887749', 'CajgKklNk')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI3', ('INSBI304740746', 'tadoHnbqL')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI3', ('INSBI304740746', 'tadoHnbqL')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI4', ('INSBI481260153', 'ScQZUIHEv')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI4', ('INSBI481260153', 'ScQZUIHEv')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI6', ('INSBI657731628', 'hNQKABNvb')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI6', ('INSBI657731628', 'hNQKABNvb')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI5', ('INSBI585529195', 'xbbjDjkVx')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI5', ('INSBI585529195', 'xbbjDjkVx')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n",
      "(('INSBI1', ('INSBI180433362', 'PvdkdMBYe')), '10.013411,200.088149,500.609548,3.174919,-10999,14.087415')\n",
      "(('INSBI1', ('INSBI180433362', 'PvdkdMBYe')), '4.013411,2.088149,5.661589,3.174919,-99999,4.087415')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 10'\n",
    "\n",
    "\n",
    "\n",
    "def insert_key(x):\n",
    "    return (x['id'][0:6],(x['id'],x['name']))\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left, x)\n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "#p = beam.Pipeline(options=options)\n",
    "\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    main_input = p|beam.io.ReadFromBigQuery(query=query,use_standard_sql=True )|beam.Map(insert_key)#|beam.Map(print))\n",
    "    \n",
    "    side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/inputs_file01.csv'))\n",
    "    \n",
    "    result = (main_input|\"Cross join\">>beam.FlatMap(cross_join, rights=beam.pvalue.AsIter(side_input)))|beam.Map(print) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "def47b2a-ab44-4b10-8a42-9336953af76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/3443825875.py:78: FutureWarning: GroupIntoBatches is experimental.\n",
      "  batches = key_insert|'Group into batches' >> beam.GroupIntoBatches(5)|beam.Map(print)\n",
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_GroupIntoBatchesDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 20'\n",
    "\n",
    "\n",
    "'''\n",
    "def insert_key(x):\n",
    "    return (x['id'][0:6],(x['id'],x['name']))\n",
    "'''\n",
    "'''\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left, x)\n",
    "'''        \n",
    "        \n",
    "'''        \n",
    "def singleton_cross_join(left, rights):\n",
    "    return (left, rights)\n",
    "'''\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={}\n",
    "        data_dict['customer_list']=element\n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        #data_dict['customer_name_list']=element[1][1]\n",
    "        #data_dict['new_sanction_name_list']=element[1][2][0].split(',')\n",
    "        \n",
    "        # The API endpoint\n",
    "        #url = \"http://35.239.64.67:32726/\"\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts/1\"\n",
    "\n",
    "        # A GET request to the API\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Print the response\n",
    "        response_json = response.json()\n",
    "       \n",
    "        length = len(element[1])\n",
    "        x = response_json['userId']\n",
    "        x1 = response_json['id']\n",
    "        x2 = response_json['title']\n",
    "        x3 = response_json['body']\n",
    "        \n",
    "        row = dict(zip(('userId','id','title','body','length'),(x,x1,x2,x3,length)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "\n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "#with beam.Pipeline(options=options) as p:\n",
    "\n",
    "main_input = (p|beam.io.ReadFromBigQuery(query=query,use_standard_sql=True ))#|beam.Map(insert_key)#|beam.Map(print))\n",
    "    \n",
    "side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/sanctioned_customer.txt'))\n",
    "    \n",
    "    #result = (main_input|\"Cross join\">>beam.FlatMap(singleton_cross_join, rights=beam.pvalue.AsSingleton(side_input)))|beam.Map(print) \n",
    "enriched = (main_input|\"Cross join\">>beam.FlatMap(cross_join, rights=beam.pvalue.AsIter(side_input)))#|beam.Map(print) \n",
    "key_insert = enriched|\"Key Insertion\">>beam.Map(insert_key)#|beam.Map(print)\n",
    "batches = key_insert|'Group into batches' >> beam.GroupIntoBatches(5)|beam.Map(print)\n",
    "'''\n",
    "process = batches|'Make an External API Call' >> beam.ParDo(make_api_call())|\"Write to Bigquery\">>beam.io.Write(beam.io.WriteToBigQuery('my-table-message-second',\n",
    "                                                                        dataset='gcpdataset',\n",
    "                                                                        project='mimetic-parity-378803',\n",
    "                                                                        schema='userId:INTEGER, id:INTEGER,title:STRING,body:STRING',\n",
    "                                                                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, #If table does not exist before, this code throws error\n",
    "                                                                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                                                                        \n",
    "                                                                      )\n",
    "                                                       )\n",
    "    \n",
    "\n",
    "'''\n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8121334c-2ef9-410b-aafe-e398e25700ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset mimetic-parity-378803:beam_temp_dataset_7e8c1034f187408e898eb4fdf9bcefa2 does not exist so we will create it as temporary with location=us-west1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('INSBI6', ('INSBI652587880', 'XqMkSHEva', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI7', ('INSBI759226449', 'hthEUEqmx', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI1', ('INSBI188022073', 'jhpicJqUN', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI2', ('INSBI230887749', 'CajgKklNk', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI5', ('INSBI585529195', 'xbbjDjkVx', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI1', ('INSBI180433362', 'PvdkdMBYe', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI4', ('INSBI481260153', 'ScQZUIHEv', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI6', ('INSBI657731628', 'hNQKABNvb', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI5', ('INSBI577879924', 'noNLOvNIo', 'yKEesKMCX,GjJFjdxaJ'))\n",
      "('INSBI3', ('INSBI304740746', 'tadoHnbqL', 'yKEesKMCX,GjJFjdxaJ'))\n"
     ]
    }
   ],
   "source": [
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 10'\n",
    "\n",
    "\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "#p = beam.Pipeline(options=options)\n",
    "\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    #with beam.Pipeline(options=options) as p:\n",
    "    main_input = (p|beam.io.ReadFromBigQuery(query=query,use_standard_sql=True ))#|beam.Map(insert_key)#|beam.Map(print))\n",
    "    side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/sanctioned_customer.txt'))\n",
    "    #result = (main_input|\"Cross join\">>beam.FlatMap(singleton_cross_join, rights=beam.pvalue.AsSingleton(side_input)))|beam.Map(print) \n",
    "    enriched = (main_input|\"Cross join\">>beam.FlatMap(cross_join, rights=beam.pvalue.AsIter(side_input)))#|beam.Map(print) \n",
    "    key_insert = enriched|\"Key Insertion\">>beam.Map(insert_key)|beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "483e4824-bea6-42de-b4e9-40d60310ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/1580589721.py:31: FutureWarning: GroupIntoBatches is experimental.\n",
      "  key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)|beam.Map(print)\n",
      "/jupyter/.kernels/apache-beam-2.44.0/lib/python3.8/site-packages/apache_beam/transforms/util.py:924: FutureWarning: GroupIntoBatches is experimental.\n",
      "  return GroupIntoBatches(*_GroupIntoBatchesParams.parse_payload(proto))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('INSBI1', [('INSBI188022073', 'jhpicJqUN', 'yKEesKMCX,GjJFjdxaJ'), ('INSBI180433362', 'PvdkdMBYe', 'yKEesKMCX,GjJFjdxaJ')])\n",
      "('INSBI6', [('INSBI652587880', 'XqMkSHEva', 'yKEesKMCX,GjJFjdxaJ'), ('INSBI657731628', 'hNQKABNvb', 'yKEesKMCX,GjJFjdxaJ')])\n",
      "('INSBI5', [('INSBI585529195', 'xbbjDjkVx', 'yKEesKMCX,GjJFjdxaJ'), ('INSBI577879924', 'noNLOvNIo', 'yKEesKMCX,GjJFjdxaJ')])\n",
      "('INSBI7', [('INSBI759226449', 'hthEUEqmx', 'yKEesKMCX,GjJFjdxaJ')])\n",
      "('INSBI2', [('INSBI230887749', 'CajgKklNk', 'yKEesKMCX,GjJFjdxaJ')])\n",
      "('INSBI4', [('INSBI481260153', 'ScQZUIHEv', 'yKEesKMCX,GjJFjdxaJ')])\n",
      "('INSBI3', [('INSBI304740746', 'tadoHnbqL', 'yKEesKMCX,GjJFjdxaJ')])\n"
     ]
    }
   ],
   "source": [
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 10'\n",
    "\n",
    "\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={}\n",
    "        data_dict['customer_list']=element\n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        #data_dict['customer_name_list']=element[1][1]\n",
    "        #data_dict['new_sanction_name_list']=element[1][2][0].split(',')\n",
    "        \n",
    "        # The API endpoint\n",
    "        #url = \"http://35.239.64.67:32726/\"\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts/1\"\n",
    "\n",
    "        # A GET request to the API\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Print the response\n",
    "        response_json = response.json()\n",
    "       \n",
    "        length = len(element[1])\n",
    "        x = response_json['userId']\n",
    "        x1 = response_json['id']\n",
    "        x2 = response_json['title']\n",
    "        x3 = response_json['body']\n",
    "        \n",
    "        row = dict(zip(('userId','id','title','body','length'),(x,x1,x2,x3,length)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "\n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "#p = beam.Pipeline(options=options)\n",
    "\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    #with beam.Pipeline(options=options) as p:\n",
    "    main_input = (p|beam.Create([('INSBI652587880', 'XqMkSHEva', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI759226449', 'hthEUEqmx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI188022073', 'jhpicJqUN', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI230887749', 'CajgKklNk', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI585529195', 'xbbjDjkVx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI180433362', 'PvdkdMBYe', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI481260153', 'ScQZUIHEv', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI657731628', 'hNQKABNvb', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI577879924', 'noNLOvNIo', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI304740746', 'tadoHnbqL', 'yKEesKMCX,GjJFjdxaJ')\n",
    "                                 ]))\n",
    "    key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)|beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6738b8e-26d9-4224-9aea-04db26fc93a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/2262143131.py:47: FutureWarning: GroupIntoBatches is experimental.\n",
      "  key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)|'Make an External API Call' >> beam.ParDo(make_api_call())|beam.Map(print)\n",
      "/jupyter/.kernels/apache-beam-2.44.0/lib/python3.8/site-packages/apache_beam/transforms/util.py:924: FutureWarning: GroupIntoBatches is experimental.\n",
      "  return GroupIntoBatches(*_GroupIntoBatchesParams.parse_payload(proto))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('INSBI188022073', 'jhpicJqUN', 'yKEesKMCX,GjJFjdxaJ'), ('INSBI180433362', 'PvdkdMBYe', 'yKEesKMCX,GjJFjdxaJ')]\n",
      "[('INSBI652587880', 'XqMkSHEva', 'yKEesKMCX,GjJFjdxaJ'), ('INSBI657731628', 'hNQKABNvb', 'yKEesKMCX,GjJFjdxaJ')]\n",
      "[('INSBI585529195', 'xbbjDjkVx', 'yKEesKMCX,GjJFjdxaJ'), ('INSBI577879924', 'noNLOvNIo', 'yKEesKMCX,GjJFjdxaJ')]\n",
      "[('INSBI759226449', 'hthEUEqmx', 'yKEesKMCX,GjJFjdxaJ')]\n",
      "[('INSBI230887749', 'CajgKklNk', 'yKEesKMCX,GjJFjdxaJ')]\n",
      "[('INSBI481260153', 'ScQZUIHEv', 'yKEesKMCX,GjJFjdxaJ')]\n",
      "[('INSBI304740746', 'tadoHnbqL', 'yKEesKMCX,GjJFjdxaJ')]\n"
     ]
    }
   ],
   "source": [
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 10'\n",
    "\n",
    "\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={}\n",
    "        data_dict['customer_list']=element\n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        \n",
    "        yield element[1] #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "\n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "#p = beam.Pipeline(options=options)\n",
    "\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    #with beam.Pipeline(options=options) as p:\n",
    "    main_input = (p|beam.Create([('INSBI652587880', 'XqMkSHEva', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI759226449', 'hthEUEqmx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI188022073', 'jhpicJqUN', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI230887749', 'CajgKklNk', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI585529195', 'xbbjDjkVx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI180433362', 'PvdkdMBYe', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI481260153', 'ScQZUIHEv', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI657731628', 'hNQKABNvb', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI577879924', 'noNLOvNIo', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI304740746', 'tadoHnbqL', 'yKEesKMCX,GjJFjdxaJ')\n",
    "                                 ]))\n",
    "    key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)|'Make an External API Call' >> beam.ParDo(make_api_call())|beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a8942343-6c12-48ac-809d-28011cc33ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/4201210615.py:70: FutureWarning: GroupIntoBatches is experimental.\n",
      "  key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)|'Make an External API Call' >> beam.ParDo(make_api_call())|beam.Map(print)\n",
      "/jupyter/.kernels/apache-beam-2.44.0/lib/python3.8/site-packages/apache_beam/transforms/util.py:924: FutureWarning: GroupIntoBatches is experimental.\n",
      "  return GroupIntoBatches(*_GroupIntoBatchesParams.parse_payload(proto))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'customer_id': ['INSBI188022073', 'INSBI180433362'], 'customer_name': ['jhpicJqUN', 'PvdkdMBYe'], 'sanctioned_name': ['yKEesKMCX', 'GjJFjdxaJ']}\n",
      "{'customer_id': ['INSBI652587880', 'INSBI657731628'], 'customer_name': ['XqMkSHEva', 'hNQKABNvb'], 'sanctioned_name': ['yKEesKMCX', 'GjJFjdxaJ']}\n",
      "{'customer_id': ['INSBI585529195', 'INSBI577879924'], 'customer_name': ['xbbjDjkVx', 'noNLOvNIo'], 'sanctioned_name': ['yKEesKMCX', 'GjJFjdxaJ']}\n",
      "{'customer_id': ['INSBI759226449'], 'customer_name': ['hthEUEqmx'], 'sanctioned_name': ['yKEesKMCX', 'GjJFjdxaJ']}\n",
      "{'customer_id': ['INSBI230887749'], 'customer_name': ['CajgKklNk'], 'sanctioned_name': ['yKEesKMCX', 'GjJFjdxaJ']}\n",
      "{'customer_id': ['INSBI481260153'], 'customer_name': ['ScQZUIHEv'], 'sanctioned_name': ['yKEesKMCX', 'GjJFjdxaJ']}\n",
      "{'customer_id': ['INSBI304740746'], 'customer_name': ['tadoHnbqL'], 'sanctioned_name': ['yKEesKMCX', 'GjJFjdxaJ']}\n"
     ]
    }
   ],
   "source": [
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 10'\n",
    "\n",
    "\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        #import urllib\n",
    "        #import urllib2\n",
    "        \n",
    "        list_c_id = []\n",
    "        list_c_name = []\n",
    "        \n",
    "        for x in element[1]:\n",
    "            list_c_id.append(x[0])\n",
    "            list_c_name.append(x[1])\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={\n",
    "                 \"customer_id\":list_c_id,\n",
    "                 \"customer_name\":list_c_name,\n",
    "                 \"sanctioned_name\":element[1][0][2].split(',')\n",
    "        }\n",
    "        #data_dict['customer_id']=list_c_id\n",
    "        #data_dict['customer_name']=list_c_name\n",
    "        #data_dict['sanctioned_name']=element[1][0][2].split(',')\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        # json object getting serialised\n",
    "        #json_object =  json.dumps(data_dict, indent = 8) #json.dumps(data_dict).encode('utf-8') \n",
    "        \n",
    "        \n",
    "        \n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        \n",
    "        yield data_dict #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "\n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "#p = beam.Pipeline(options=options)\n",
    "\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    #with beam.Pipeline(options=options) as p:\n",
    "    main_input = (p|beam.Create([('INSBI652587880', 'XqMkSHEva', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI759226449', 'hthEUEqmx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI188022073', 'jhpicJqUN', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI230887749', 'CajgKklNk', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI585529195', 'xbbjDjkVx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI180433362', 'PvdkdMBYe', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI481260153', 'ScQZUIHEv', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI657731628', 'hNQKABNvb', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI577879924', 'noNLOvNIo', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI304740746', 'tadoHnbqL', 'yKEesKMCX,GjJFjdxaJ')\n",
    "                                 ]))\n",
    "    key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)|'Make an External API Call' >> beam.ParDo(make_api_call())|beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7802b98e-dad4-4aee-a9ec-751ff94358db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/834015840.py:72: FutureWarning: GroupIntoBatches is experimental.\n",
      "  batches = key_insert|'Group into batches' >> beam.GroupIntoBatches(5)#|beam.Map(print)\n",
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_GroupIntoBatchesDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 20'\n",
    "\n",
    "\n",
    "'''\n",
    "def insert_key(x):\n",
    "    return (x['id'][0:6],(x['id'],x['name']))\n",
    "'''\n",
    "'''\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left, x)\n",
    "'''        \n",
    "        \n",
    "'''        \n",
    "def singleton_cross_join(left, rights):\n",
    "    return (left, rights)\n",
    "'''\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        #import urllib\n",
    "        #import urllib2\n",
    "        \n",
    "        list_c_id = []\n",
    "        list_c_name = []\n",
    "        \n",
    "        for x in element[1]:\n",
    "            list_c_id.append(x[0])\n",
    "            list_c_name.append(x[1])\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={\n",
    "                 \"customer_id\":list_c_id,\n",
    "                 \"customer_name\":list_c_name,\n",
    "                 \"sanctioned_name\":element[1][0][2].split(',')\n",
    "        }\n",
    "        #data_dict['customer_id']=list_c_id\n",
    "        #data_dict['customer_name']=list_c_name\n",
    "        #data_dict['sanctioned_name']=element[1][0][2].split(',')\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'      \n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        x = post_response.text\n",
    "        row = dict(zip(('Message','id'),(x,0)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "\n",
    "        \n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "import apache_beam as beam \n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "main_input = (p|beam.io.ReadFromBigQuery(query=query,use_standard_sql=True ))#|beam.Map(insert_key)#|beam.Map(print))    \n",
    "side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/sanctioned_customer.txt'))\n",
    "enriched = (main_input|\"Cross join\">>beam.FlatMap(cross_join, rights=beam.pvalue.AsIter(side_input)))#|beam.Map(print) \n",
    "key_insert = enriched|\"Key Insertion\">>beam.Map(insert_key)#|beam.Map(print)\n",
    "batches = key_insert|'Group into batches' >> beam.GroupIntoBatches(5)#|beam.Map(print)\n",
    "process = batches|'Make an External API Call' >> beam.ParDo(make_api_call())|\"Write to Bigquery\">>beam.io.Write(beam.io.WriteToBigQuery('my-table-message-third',\n",
    "                                                                        dataset='gcpdataset',\n",
    "                                                                        project='mimetic-parity-378803',\n",
    "                                                                        schema='Message:STRING,id:INTEGER',\n",
    "                                                                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, #If table does not exist before, this code throws error\n",
    "                                                                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                                                                        \n",
    "                                                                      )\n",
    "                                                       )\n",
    "    \n",
    "\n",
    "\n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ad82d6ed-f107-42f9-a45d-7c76b6a33eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/3183263075.py:71: FutureWarning: GroupIntoBatches is experimental.\n",
      "  key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)|'Make an External API Call' >> beam.ParDo(make_api_call())|beam.Map(print)\n",
      "/jupyter/.kernels/apache-beam-2.44.0/lib/python3.8/site-packages/apache_beam/transforms/util.py:924: FutureWarning: GroupIntoBatches is experimental.\n",
      "  return GroupIntoBatches(*_GroupIntoBatchesParams.parse_payload(proto))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Message': '<h1>invalid credentials!</h1>', 'id': 0}\n",
      "{'Message': '<h1>invalid credentials!</h1>', 'id': 0}\n",
      "{'Message': '<h1>invalid credentials!</h1>', 'id': 0}\n",
      "{'Message': '<h1>invalid credentials!</h1>', 'id': 0}\n",
      "{'Message': '<h1>invalid credentials!</h1>', 'id': 0}\n",
      "{'Message': '<h1>invalid credentials!</h1>', 'id': 0}\n",
      "{'Message': '<h1>invalid credentials!</h1>', 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 10'\n",
    "\n",
    "\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        #import urllib\n",
    "        #import urllib2\n",
    "        \n",
    "        list_c_id = []\n",
    "        list_c_name = []\n",
    "        \n",
    "        for x in element[1]:\n",
    "            list_c_id.append(x[0])\n",
    "            list_c_name.append(x[1])\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={\n",
    "                 \"customer_id\":list_c_id,\n",
    "                 \"customer_name\":list_c_name,\n",
    "                 \"sanctioned_name\":element[1][0][2].split(',')\n",
    "        }\n",
    "        #data_dict['customer_id']=list_c_id\n",
    "        #data_dict['customer_name']=list_c_name\n",
    "        #data_dict['sanctioned_name']=element[1][0][2].split(',')\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        # json object getting serialised\n",
    "        #json_object =  json.dumps(data_dict, indent = 8) #json.dumps(data_dict).encode('utf-8') \n",
    "        \n",
    "        \n",
    "        \n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        x = post_response.text\n",
    "        row = dict(zip(('Message','id'),(x,0)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "\n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "#p = beam.Pipeline(options=options)\n",
    "\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    #with beam.Pipeline(options=options) as p:\n",
    "    main_input = (p|beam.Create([('INSBI652587880', 'XqMkSHEva', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI759226449', 'hthEUEqmx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI188022073', 'jhpicJqUN', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI230887749', 'CajgKklNk', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI585529195', 'xbbjDjkVx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI180433362', 'PvdkdMBYe', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI481260153', 'ScQZUIHEv', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI657731628', 'hNQKABNvb', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI577879924', 'noNLOvNIo', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI304740746', 'tadoHnbqL', 'yKEesKMCX,GjJFjdxaJ')\n",
    "                                 ]))\n",
    "    key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)|'Make an External API Call' >> beam.ParDo(make_api_call())|beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c39df0fb-9a9e-4a9a-beb0-99eb3df5d766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/2343147345.py:72: FutureWarning: GroupIntoBatches is experimental.\n",
      "  batches = key_insert|'Group into batches' >> beam.GroupIntoBatches(3000)#|beam.Map(print)\n",
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_GroupIntoBatchesDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records`'\n",
    "\n",
    "\n",
    "'''\n",
    "def insert_key(x):\n",
    "    return (x['id'][0:6],(x['id'],x['name']))\n",
    "'''\n",
    "'''\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left, x)\n",
    "'''        \n",
    "        \n",
    "'''        \n",
    "def singleton_cross_join(left, rights):\n",
    "    return (left, rights)\n",
    "'''\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        #import urllib\n",
    "        #import urllib2\n",
    "        \n",
    "        list_c_id = []\n",
    "        list_c_name = []\n",
    "        \n",
    "        for x in element[1]:\n",
    "            list_c_id.append(x[0])\n",
    "            list_c_name.append(x[1])\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={\n",
    "                 \"customer_id\":list_c_id,\n",
    "                 \"customer_name\":list_c_name,\n",
    "                 \"sanctioned_name\":element[1][0][2].split(',')\n",
    "        }\n",
    "        #data_dict['customer_id']=list_c_id\n",
    "        #data_dict['customer_name']=list_c_name\n",
    "        #data_dict['sanctioned_name']=element[1][0][2].split(',')\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'      \n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        x = post_response.text\n",
    "        row = dict(zip(('Message','id'),(x,0)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "\n",
    "        \n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "import apache_beam as beam \n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "main_input = (p|beam.io.ReadFromBigQuery(query=query,use_standard_sql=True ))#|beam.Map(insert_key)#|beam.Map(print))    \n",
    "side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/sanctioned_customer.txt'))\n",
    "enriched = (main_input|\"Cross join\">>beam.FlatMap(cross_join, rights=beam.pvalue.AsIter(side_input)))#|beam.Map(print) \n",
    "key_insert = enriched|\"Key Insertion\">>beam.Map(insert_key)#|beam.Map(print)\n",
    "batches = key_insert|'Group into batches' >> beam.GroupIntoBatches(8000)#|beam.Map(print)\n",
    "process = batches|'Make an External API Call' >> beam.ParDo(make_api_call())|\"Write to Bigquery\">>beam.io.Write(beam.io.WriteToBigQuery('my-table-message-third',\n",
    "                                                                        dataset='gcpdataset',\n",
    "                                                                        project='mimetic-parity-378803',\n",
    "                                                                        schema='Message:STRING,id:INTEGER',\n",
    "                                                                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, #If table does not exist before, this code throws error\n",
    "                                                                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                                                                        \n",
    "                                                                      )\n",
    "                                                       )\n",
    "    \n",
    "\n",
    "\n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6d50e41f-f8a1-45c0-a3d1-421fd32b2dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/3366851488.py:113: FutureWarning: GroupIntoBatches is experimental.\n",
      "  |'Group into batches' >> beam.GroupIntoBatches(50000)\n",
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_GroupIntoBatchesDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Create the Options first\n",
    "\n",
    "\n",
    "import google.auth\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "#from apache_beam.utils.pipeline_options import SetupOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import WorkerOptions\n",
    "import apache_beam as beam\n",
    "import time\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "\n",
    "options = PipelineOptions()\n",
    "\n",
    "#Setting the Options Programmatically\n",
    "options = PipelineOptions(flags=[])\n",
    "\n",
    "#set the project to the default project in your current Google Cloud Environment\n",
    "_, options.view_as(GoogleCloudOptions).project=google.auth.default()\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(StandardOptions).streaming = True\n",
    "options.view_as(GoogleCloudOptions).enable_streaming_engine = True\n",
    "#options.view_as(PipelineOptions).experiments = 'use_runner_v2' the log says it is by deafult\n",
    "\n",
    "#number of workers\n",
    "options.view_as(WorkerOptions).num_workers = 2\n",
    "\n",
    "#Set the Google Cloud Region in which Cloud Dataflow run\n",
    "options.view_as(GoogleCloudOptions).region='us-west1'\n",
    "\n",
    "#Cloud Storage Location\n",
    "dataflow_gcs_location='gs://gcp-dataeng-demos-soumya/dataflow'\n",
    "\n",
    "#Dataflow Staging Location. This location is used to stage the Dataflow Pipeline and SDK binary\n",
    "options.view_as(GoogleCloudOptions).staging_location = '{}/staging'.format(dataflow_gcs_location)\n",
    "#Dataflow Temp Location. This location is used to store temporary files or intermediate results before finally outputting\n",
    "options.view_as(GoogleCloudOptions).temp_location = '{}/temp'.format(dataflow_gcs_location)\n",
    "#The directory to store the output files of the job\n",
    "output_gcs_location = '{}/output'.format(dataflow_gcs_location)\n",
    "\n",
    "\n",
    "########################################################################  With A Single Worker    ############################################################################\n",
    "\n",
    "\n",
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records`'\n",
    "\n",
    "\n",
    "\n",
    "def insert_key(x):\n",
    "    return (x['id'][0:6],(x['id'],x['name']))\n",
    "\n",
    "def makeRow(x):\n",
    "    row = dict(zip(('id','name'),x))\n",
    "    return row\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element, right):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        #from kafka import KafkaProducer\n",
    "        \n",
    "        #producer = KafkaProducer(bootstrap_servers=['34.28.118.32:9094'], api_version=(0, 10))\n",
    "        \n",
    "        #content = urllib.request.urlopen('http://35.239.64.67:32726/').read().decode('utf-8')\n",
    "        #row = dict(zip(('message'),content))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # The API endpoint\n",
    "        #url = \"http://35.239.64.67:32726/\"\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts/1\"\n",
    "\n",
    "        # A GET request to the API\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Print the response\n",
    "        response_json = response.json()\n",
    "        #producer.send('my-topic', json.dumps(response_json).encode('utf-8'))\n",
    "        #if producer is not None:\n",
    "                    #producer.close()\n",
    "\n",
    "        length = len(element[1])\n",
    "        x = response_json['userId']\n",
    "        x1 = response_json['id']\n",
    "        x2 = response_json['title']\n",
    "        x3 = response_json['body']\n",
    "        #x=1\n",
    "        #x1=1\n",
    "        #x2='string'\n",
    "        #x3='strong'\n",
    "        \n",
    "        row = dict(zip(('userId','id','title','body','length'),(x,x1,x2,x3,length)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "          \n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "#with beam.Pipeline(options=options) as p:\n",
    "side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/sanctioned_customer.txt'))\n",
    "read_requests =(p| \n",
    "    beam.io.ReadFromBigQuery(query=query,use_standard_sql=True )\n",
    "    |beam.Map(insert_key)\n",
    "    |'Group into batches' >> beam.GroupIntoBatches(50000)\n",
    "    |'Make an External API Call' >> beam.ParDo(make_api_call(),beam.pvalue.AsIter(side_input))\n",
    "    |\"Write to Bigquery\">>beam.io.Write(beam.io.WriteToBigQuery(        'my-table-message-second',\n",
    "                                                                        dataset='gcpdataset',\n",
    "                                                                        project='mimetic-parity-378803',\n",
    "                                                                        schema='userId:INTEGER, id:INTEGER,title:STRING,body:STRING,length:INTEGER',\n",
    "                                                                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, #If table does not exist before, this code throws error\n",
    "                                                                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                                                                        \n",
    "                                                                      )\n",
    "                                                       )\n",
    "    )\n",
    "    #|beam.Map(print))\n",
    "               \n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c0248edb-9b70-42da-baba-2d968d3e1a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/3764640179.py:74: FutureWarning: GroupIntoBatches is experimental.\n",
      "  key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)#|beam.Map(print)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records` LIMIT 10'\n",
    "\n",
    "\n",
    "\n",
    "def cross_join(left, rights):\n",
    "    for x in rights:\n",
    "        yield (left['id'],left['name'],x)\n",
    "        \n",
    "def insert_key(x):\n",
    "    return (x[0][0:6],(x[0],x[1],x[2]))\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element, side_input):\n",
    "        \n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        #import urllib\n",
    "        #import urllib2\n",
    "        \n",
    "        list_c_id = []\n",
    "        list_c_name = []\n",
    "        \n",
    "        for x in element[1]:\n",
    "            list_c_id.append(x[0])\n",
    "            list_c_name.append(x[1])\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={\n",
    "                 \"customer_id\":list_c_id,\n",
    "                 \"customer_name\":list_c_name,\n",
    "                 \"sanctioned_name\":side_input.split(',')\n",
    "        }\n",
    "        #data_dict['customer_id']=list_c_id\n",
    "        #data_dict['customer_name']=list_c_name\n",
    "        #data_dict['sanctioned_name']=element[1][0][2].split(',')\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'      \n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        x = post_response.text\n",
    "        row = dict(zip(('Message','id'),(x,0)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "\n",
    "\n",
    "def log_element(element):\n",
    "    logging.info(element)\n",
    "    return element\n",
    "\n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "import logging\n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "#with beam.Pipeline(options=options) as p:\n",
    "side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/sanctioned_customer.txt'))\n",
    "#with beam.Pipeline(options=options) as p:\n",
    "#side_input = (p|beam.Create(['yKEesKMCX,GjJFjdxaJ,XqMkSHEva,ScQZUIHEv']))\n",
    "main_input = (p|beam.Create([('INSBI652587880', 'XqMkSHEva', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI759226449', 'hthEUEqmx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI188022073', 'jhpicJqUN', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI230887749', 'CajgKklNk', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI585529195', 'xbbjDjkVx', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI180433362', 'PvdkdMBYe', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI481260153', 'ScQZUIHEv', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI657731628', 'hNQKABNvb', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI577879924', 'noNLOvNIo', 'yKEesKMCX,GjJFjdxaJ'),\n",
    "                                 ('INSBI304740746', 'tadoHnbqL', 'yKEesKMCX,GjJFjdxaJ')\n",
    "                                 ]))\n",
    "key_insert = main_input|\"Key Insertion\">>beam.Map(insert_key)| 'Group into batches' >> beam.GroupIntoBatches(2)#|beam.Map(print)\n",
    "api_call =  key_insert|'Make an External API Call' >> beam.ParDo(make_api_call(),beam.pvalue.AsSingleton(side_input))|beam.Map(log_element)#beam.Map(print)\n",
    "#|'Make an External API Call' >> beam.ParDo(make_api_call())|beam.Map(print)\n",
    "    \n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options).wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9ec54a2c-75fc-48f3-b542-65ce72aba835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/1345512093.py:118: FutureWarning: GroupIntoBatches is experimental.\n",
      "  |'Group into batches' >> beam.GroupIntoBatches(50000)\n",
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_GroupIntoBatchesDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Create the Options first\n",
    "\n",
    "\n",
    "import google.auth\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "#from apache_beam.utils.pipeline_options import SetupOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import WorkerOptions\n",
    "import apache_beam as beam\n",
    "import time\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "\n",
    "options = PipelineOptions()\n",
    "\n",
    "#Setting the Options Programmatically\n",
    "options = PipelineOptions(flags=[])\n",
    "\n",
    "#set the project to the default project in your current Google Cloud Environment\n",
    "_, options.view_as(GoogleCloudOptions).project=google.auth.default()\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(StandardOptions).streaming = True\n",
    "options.view_as(GoogleCloudOptions).enable_streaming_engine = True\n",
    "#options.view_as(PipelineOptions).experiments = 'use_runner_v2' the log says it is by deafult\n",
    "\n",
    "#number of workers\n",
    "options.view_as(WorkerOptions).num_workers = 2\n",
    "\n",
    "#Set the Google Cloud Region in which Cloud Dataflow run\n",
    "options.view_as(GoogleCloudOptions).region='us-west1'\n",
    "\n",
    "#Cloud Storage Location\n",
    "dataflow_gcs_location='gs://gcp-dataeng-demos-soumya/dataflow'\n",
    "\n",
    "#Dataflow Staging Location. This location is used to stage the Dataflow Pipeline and SDK binary\n",
    "options.view_as(GoogleCloudOptions).staging_location = '{}/staging'.format(dataflow_gcs_location)\n",
    "#Dataflow Temp Location. This location is used to store temporary files or intermediate results before finally outputting\n",
    "options.view_as(GoogleCloudOptions).temp_location = '{}/temp'.format(dataflow_gcs_location)\n",
    "#The directory to store the output files of the job\n",
    "output_gcs_location = '{}/output'.format(dataflow_gcs_location)\n",
    "\n",
    "\n",
    "########################################################################  With A Single Worker    ############################################################################\n",
    "\n",
    "\n",
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records`'\n",
    "\n",
    "\n",
    "\n",
    "def insert_key(x):\n",
    "    return (x['id'][0:6],(x['id'],x['name']))\n",
    "\n",
    "def makeRow(x):\n",
    "    row = dict(zip(('id','name'),x))\n",
    "    return row\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element, right):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        #from kafka import KafkaProducer\n",
    "        \n",
    "        #producer = KafkaProducer(bootstrap_servers=['34.28.118.32:9094'], api_version=(0, 10))\n",
    "        \n",
    "        #content = urllib.request.urlopen('http://35.239.64.67:32726/').read().decode('utf-8')\n",
    "        #row = dict(zip(('message'),content))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # The API endpoint\n",
    "        #url = \"http://35.239.64.67:32726/\"\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts/1\"\n",
    "\n",
    "        # A GET request to the API\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Print the response\n",
    "        response_json = response.json()\n",
    "        #producer.send('my-topic', json.dumps(response_json).encode('utf-8'))\n",
    "        #if producer is not None:\n",
    "                    #producer.close()\n",
    "\n",
    "        length = len(element[1])\n",
    "        x = response_json['userId']\n",
    "        x1 = response_json['id']\n",
    "        x2 = response_json['title']\n",
    "        x3 = response_json['body']\n",
    "        #x=1\n",
    "        #x1=1\n",
    "        #x2='string'\n",
    "        #x3='strong'\n",
    "        \n",
    "        row = dict(zip(('userId','id','title','body','length'),(x,x1,x2,x3,length)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "          \n",
    "def log_element(element):\n",
    "    logging.info(element)\n",
    "    return element\n",
    "            \n",
    "            \n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "import logging\n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "#with beam.Pipeline(options=options) as p:\n",
    "side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/sanctioned_customer.txt'))\n",
    "read_requests =(p| \n",
    "    beam.io.ReadFromBigQuery(query=query,use_standard_sql=True )\n",
    "    |beam.Map(insert_key)\n",
    "    |'Group into batches' >> beam.GroupIntoBatches(50000)\n",
    "    |'Make an External API Call' >> beam.ParDo(make_api_call(),beam.pvalue.AsSingleton(side_input))\n",
    "    |\"Write to Bigquery\">>beam.io.Write(beam.io.WriteToBigQuery(        'my-table-message-second',\n",
    "                                                                        dataset='gcpdataset',\n",
    "                                                                        project='mimetic-parity-378803',\n",
    "                                                                        schema='userId:INTEGER, id:INTEGER,title:STRING,body:STRING,length:INTEGER',\n",
    "                                                                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, #If table does not exist before, this code throws error\n",
    "                                                                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                                                                        \n",
    "                                                                      )\n",
    "                                                       )\n",
    "    )\n",
    "    #|beam.Map(print))\n",
    "               \n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "91054863-1cb8-4c20-a7fa-f477f0324d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3444/103542436.py:110: FutureWarning: GroupIntoBatches is experimental.\n",
      "  |'Group into batches' >> beam.GroupIntoBatches(10000)\n",
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_GroupIntoBatchesDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: /jupyter/.kernels/apache-beam-2.44.0/bin/python -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Create the Options first\n",
    "\n",
    "\n",
    "import google.auth\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "#from apache_beam.utils.pipeline_options import SetupOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import WorkerOptions\n",
    "import apache_beam as beam\n",
    "import time\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "\n",
    "options = PipelineOptions()\n",
    "\n",
    "#Setting the Options Programmatically\n",
    "options = PipelineOptions(flags=[])\n",
    "\n",
    "#set the project to the default project in your current Google Cloud Environment\n",
    "_, options.view_as(GoogleCloudOptions).project=google.auth.default()\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(StandardOptions).streaming = True\n",
    "options.view_as(GoogleCloudOptions).enable_streaming_engine = True\n",
    "#options.view_as(PipelineOptions).experiments = 'use_runner_v2' the log says it is by deafult\n",
    "\n",
    "#number of workers\n",
    "options.view_as(WorkerOptions).num_workers = 2\n",
    "\n",
    "#Set the Google Cloud Region in which Cloud Dataflow run\n",
    "options.view_as(GoogleCloudOptions).region='us-west1'\n",
    "\n",
    "#Cloud Storage Location\n",
    "dataflow_gcs_location='gs://gcp-dataeng-demos-soumya/dataflow'\n",
    "\n",
    "#Dataflow Staging Location. This location is used to stage the Dataflow Pipeline and SDK binary\n",
    "options.view_as(GoogleCloudOptions).staging_location = '{}/staging'.format(dataflow_gcs_location)\n",
    "#Dataflow Temp Location. This location is used to store temporary files or intermediate results before finally outputting\n",
    "options.view_as(GoogleCloudOptions).temp_location = '{}/temp'.format(dataflow_gcs_location)\n",
    "#The directory to store the output files of the job\n",
    "output_gcs_location = '{}/output'.format(dataflow_gcs_location)\n",
    "\n",
    "\n",
    "########################################################################  With A Single Worker    ############################################################################\n",
    "\n",
    "\n",
    "query='SELECT id,name FROM `mimetic-parity-378803.gcpdataset.my-table-customer-records`'\n",
    "\n",
    "\n",
    "\n",
    "def insert_key(x):\n",
    "    return (x['id'][0:6],(x['id'],x['name']))\n",
    "\n",
    "def makeRow(x):\n",
    "    row = dict(zip(('id','name'),x))\n",
    "    return row\n",
    "\n",
    "class make_api_call(beam.DoFn):\n",
    "    \n",
    "    def process(self, element, side_input):\n",
    "        # importing the requests library\n",
    "        import requests\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        #import urllib\n",
    "        #import urllib2\n",
    "        \n",
    "        list_c_id = []\n",
    "        list_c_name = []\n",
    "        \n",
    "        for x in element[1]:\n",
    "            list_c_id.append(x[0])\n",
    "            list_c_name.append(x[1])\n",
    "        \n",
    "        url_post = 'http://35.192.38.85:80/hello/post'\n",
    "        data_dict={\n",
    "                 \"customer_id\":list_c_id,\n",
    "                 \"customer_name\":list_c_name,\n",
    "                 \"sanctioned_name\":side_input.split(',')\n",
    "        }\n",
    "        #data_dict['customer_id']=list_c_id\n",
    "        #data_dict['customer_name']=list_c_name\n",
    "        #data_dict['sanctioned_name']=element[1][0][2].split(',')\n",
    "        length = len(element[1])\n",
    "        url_post = 'http://35.192.38.85:80/hello/post'      \n",
    "        post_response = requests.post(url_post, data = data_dict)\n",
    "        x = post_response.text\n",
    "        row = dict(zip(('Message','id','Batchsize'),(x,0,length)))\n",
    "        yield row #A return here was not working, please remember, from error message [row[0].json cannot be done], so the write to bigquery expects an interator\n",
    "          \n",
    "def log_element(element):\n",
    "    logging.info(element)\n",
    "    return element\n",
    "            \n",
    "            \n",
    "from apache_beam.io.gcp.bigquery import  ReadFromBigQueryRequest, ReadAllFromBigQuery\n",
    "\n",
    "import apache_beam as beam \n",
    "import logging\n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "#with beam.Pipeline(options=options) as p:\n",
    "side_input = (p|\"Side Input From GCS\" >> beam.io.ReadFromText('gs://gcp-dataeng-demos-soumya/dataflow/sanctioned_customer.txt'))\n",
    "read_requests =(p| \n",
    "    beam.io.ReadFromBigQuery(query=query,use_standard_sql=True )\n",
    "    |beam.Map(insert_key)\n",
    "    |'Group into batches' >> beam.GroupIntoBatches(10000)\n",
    "    |'Make an External API Call' >> beam.ParDo(make_api_call(),beam.pvalue.AsSingleton(side_input))\n",
    "    |\"Write to Bigquery\">>beam.io.Write(beam.io.WriteToBigQuery('my-table-message-fourth',\n",
    "                                                                        dataset='gcpdataset',\n",
    "                                                                        project='mimetic-parity-378803',\n",
    "                                                                        schema='Message:STRING,id:INTEGER,Batchsize:INTEGER',\n",
    "                                                                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, #If table does not exist before, this code throws error\n",
    "                                                                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                                                                        \n",
    "                                                                      )\n",
    "    )\n",
    "               )\n",
    "    #|beam.Map(print))\n",
    "               \n",
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbde4bf-5cea-4103-a7ff-cc6153053003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.44.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.44.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
